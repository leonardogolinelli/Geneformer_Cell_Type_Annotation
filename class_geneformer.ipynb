{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "import scipy.sparse as sp\n",
    "import datetime\n",
    "from geneformer import Classifier\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Define the URL and output directory\n",
    "url = \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/gf-6L-30M-i2048/model.safetensors\"\n",
    "output_dir = \"./gf-6L-30M-i2048/\"\n",
    "output_file = os.path.join(output_dir, \"model.safetensors\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(output_file):\n",
    "    print(f\"Downloading {output_file}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download completed.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file: HTTP {response.status_code}\")\n",
    "else:\n",
    "    print(f\"File already exists: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/\"\n",
    "files = [\n",
    "    \"ensembl_mapping_dict_gc30M.pkl\",\n",
    "    \"gene_median_dictionary_gc30M.pkl\",\n",
    "    \"gene_name_id_dict_gc30M.pkl\",\n",
    "    \"token_dictionary_gc30M.pkl\"\n",
    "]\n",
    "\n",
    "output_dir = \"./gene_dictionaries_30m\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    output_file = os.path.join(output_dir, file)\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Downloading {file}...\")\n",
    "        urllib.request.urlretrieve(base_url + file, output_file)\n",
    "        print(f\"Downloaded {file}\")\n",
    "    else:\n",
    "        print(f\"{file} already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cell_type_distribution(cell_types):\n",
    "    # Count occurrences of each class\n",
    "    class_distribution = Counter(cell_types)\n",
    "\n",
    "    # Calculate relative frequencies\n",
    "    total_count = sum(class_distribution.values())\n",
    "    class_relative_frequencies = {key: value / total_count for key, value in class_distribution.items()}\n",
    "\n",
    "    # Extract keys and values for plotting\n",
    "    class_names = list(class_relative_frequencies.keys())\n",
    "    class_frequencies = list(class_relative_frequencies.values())\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(class_names, class_frequencies)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Cell Types', fontsize=12)\n",
    "    plt.ylabel('Relative Frequency', fontsize=12)\n",
    "    plt.title('Cell Type Distribution (Relative Frequencies)', fontsize=14)\n",
    "\n",
    "    # Rotate x-axis labels for better readability if needed\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    # Print relative frequencies for verification\n",
    "    for key, value in class_distribution.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "cell_file = \"data/cells.npy\"\n",
    "cells = np.load(cell_file, allow_pickle=True).ravel()[0]\n",
    "\n",
    "# Extract data\n",
    "expressions = cells[\"UMI\"].toarray()  # Gene expression matrix (n_cells x n_genes)\n",
    "gene_names = cells[\"gene_ids\"]  # Gene names\n",
    "cell_types = cells[\"classes\"]  # Cell types (n_cells,)\n",
    "\n",
    "plot_cell_type_distribution(cell_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for stratified sampling\n",
    "cell_df = pd.DataFrame({\"cell_types\": cell_types})\n",
    "\n",
    "# Perform stratified sampling to select 10% of the data\n",
    "_, subsample_indices = train_test_split(\n",
    "    np.arange(len(cell_types)),  # Use indices for subsampling\n",
    "    test_size=0.01,  # 10% subsample\n",
    "    stratify=cell_df[\"cell_types\"],  # Stratify by cell types\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Original dataset size: {len(cell_types)}\")\n",
    "\n",
    "# Subset the data based on sampled indices\n",
    "expressions = expressions[subsample_indices, :]  # Subset expression matrix\n",
    "cell_types = cell_types[subsample_indices]  # Subset cell types\n",
    "# Output sizes for verification\n",
    "print(f\"Subsampled dataset size: {len(cell_types)}\")\n",
    "\n",
    "plot_cell_type_distribution(cell_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.sparse as sp\n",
    "import scanpy as sc\n",
    "\n",
    "# Example data\n",
    "# Replace `expressions`, `cell_types`, and `gene_names` with your actual data\n",
    "adata = sc.AnnData(X=expressions)\n",
    "adata.obs[\"cell_types\"] = cell_types\n",
    "adata.var_names = gene_names\n",
    "adata.var[\"ensembl_id\"] = gene_names\n",
    "adata.obs[\"n_counts\"] = adata.X.sum(1)  # total read count per cell\n",
    "adata.obs[\"cell_id\"] = adata.obs_names.values\n",
    "adata.obs[\"label\"] = adata.obs[\"cell_types\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Convert matrix to sparse format if not already\n",
    "if not sp.issparse(adata.X):\n",
    "    adata.X = sp.csr_matrix(adata.X)\n",
    "\n",
    "# Save the AnnData object\n",
    "adata.write_h5ad(\"data/adata.h5ad\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer import TranscriptomeTokenizer\n",
    "\n",
    "tokenizer = TranscriptomeTokenizer(\n",
    "    custom_attr_name_dict={\"cell_types\": \"cell_types\", \"cell_id\": \"cell_id\", \"label\":\"class_id\"},#,\"class_id\":\"class_id\"},\n",
    "    model_input_size=2048,  # For 30M model series\n",
    "    special_token=False,   # 30M models require this to be False\n",
    "    gene_median_file=\"./gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl\",\n",
    "    token_dictionary_file=\"./gene_dictionaries_30m/token_dictionary_gc30M.pkl\",\n",
    "    gene_mapping_file=\"./gene_dictionaries_30m/ensembl_mapping_dict_gc30M.pkl\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.tokenize_data(\n",
    "    data_directory=\"./data\",\n",
    "    output_directory=\"./tokenized_data\",\n",
    "    output_prefix=\"my_dataset\",\n",
    "    file_format=\"h5ad\",\n",
    "    use_generator=False\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Example: Map cell types to numeric IDs\n",
    "cell_types_v2 = list(adata.obs[\"cell_types\"].unique())\n",
    "id_class_dict = {i: class_id for i, class_id in enumerate(cell_types_v2)}\n",
    "\n",
    "# Save the dictionary\n",
    "with open(\"./tokenized_data/my_dataset_id_class_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_class_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from datasets import Dataset\n",
    "\n",
    "# Load the tokenized dataset\n",
    "tokenized_data = Dataset.load_from_disk(\"./tokenized_data/my_dataset.dataset\")\n",
    "\n",
    "# Map the original labels back to the tokenized dataset\n",
    "labels = adata.obs[\"class_id\"].values\n",
    "tokenized_data = tokenized_data.map(lambda x, idx: {\"class_id\": labels[idx]}, with_indices=True)\n",
    "\n",
    "# Save back the tokenized dataset with the added labels\n",
    "tokenized_data.save_to_disk(\"./tokenized_data/my_dataset_with_labels.dataset\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}{current_date.hour:02d}{current_date.minute:02d}{current_date.second:02d}\"\n",
    "datestamp_min = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "\n",
    "output_prefix = \"cm_classifier_test\"\n",
    "output_dir = f\"output_directory/{datestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data_dict={\"cell_types\":list(adata.obs[\"cell_types\"].unique())}\n",
    "training_args = {\n",
    "    \"num_train_epochs\": 0.9,\n",
    "    \"learning_rate\": 0.000804,\n",
    "    \"lr_scheduler_type\": \"polynomial\",\n",
    "    \"warmup_steps\": 1812,\n",
    "    \"weight_decay\":0.258828,\n",
    "    \"per_device_train_batch_size\": 12,\n",
    "    \"seed\": 73,\n",
    "}\n",
    "cc = Classifier(classifier=\"cell\",\n",
    "                cell_state_dict = {\"state_key\": \"cell_types\", \"states\": \"all\"},\n",
    "                filter_data=None, #none = fine tune with all input data\n",
    "                training_args=training_args,\n",
    "                max_ncells=None,\n",
    "                freeze_layers = 2, # freeze the last 2 layer of the model\n",
    "                num_crossval_splits = 1, #only 1 train, test and eval. no cross validation\n",
    "                forward_batch_size=200,\n",
    "                nproc=1,)\n",
    "                #rare_threshold=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    np.arange(len(cell_types)), test_size=0.3, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# Step 2: Split temp (30%) into validation (15%) and test (15%)\n",
    "eval_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# Now get the cell IDs corresponding to these indices\n",
    "train_ids = adata.obs['cell_id'][train_indices].tolist()\n",
    "eval_ids = adata.obs['cell_id'][eval_indices].tolist()\n",
    "test_ids = adata.obs['cell_id'][test_indices].tolist()\n",
    "\n",
    "# Output the sizes\n",
    "print(f\"Total samples: {len(cell_types)}\")\n",
    "print(f\"Training samples: {len(train_ids)}\")\n",
    "print(f\"Validation samples: {len(eval_ids)}\")\n",
    "print(f\"Test samples: {len(test_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_id_split_dict = {\"attr_key\": \"cell_id\",\n",
    "                            \"train\": train_ids+eval_ids,\n",
    "                            \"test\": test_ids}\n",
    "\n",
    "# Example input_data_file: https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/example_input_files/cell_classification/disease_classification/human_dcm_hcm_nf.dataset\n",
    "cc.prepare_data(input_data_file=f\"tokenized_data/my_dataset.dataset/\",\n",
    "                output_directory=output_dir,\n",
    "                output_prefix=output_prefix,\n",
    "                split_id_dict=train_test_id_split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Path to your tokenized dataset\n",
    "dataset_path = \"./tokenized_data/my_dataset.dataset\"\n",
    "\n",
    "# Load the dataset\n",
    "tokenized_data = load_from_disk(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_number = \"5\"\n",
    "os.makedirs(f\"results/{test_number}\", exist_ok=True)\n",
    "\"\"\"train_valid_id_split_dict = {\"attr_key\": \"cell_types\",\n",
    "                            \"train\": train_ids,\n",
    "                            \"eval\": eval_ids}\"\"\"\n",
    "\n",
    "train_valid_id_split_dict = {\"attr_key\": \"class_id\", \"train\": train_ids, \"eval\": eval_ids}\n",
    "\n",
    "# 6 layer Geneformer: https://huggingface.co/ctheodoris/Geneformer/blob/main/model.safetensors\n",
    "all_metrics = cc.validate(\n",
    "                model_directory=\"gf-6L-30M-i2048\",\n",
    "                prepared_input_data_file=\"./tokenized_data/my_dataset.dataset\",\n",
    "                id_class_dict_file=\"./tokenized_data/my_dataset_id_class_dict.pkl\",\n",
    "                output_directory=f\"./results/{test_number}\",\n",
    "                output_prefix=\"my_fine_tuned_model\",\n",
    "                split_id_dict=train_valid_id_split_dict\n",
    "            )\n",
    "                          # to optimize hyperparameters, set n_hyperopt_trials=100 (or alternative desired # of trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
