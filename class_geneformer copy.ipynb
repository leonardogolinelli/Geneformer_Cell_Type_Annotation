{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "import scipy.sparse as sp\n",
    "import datetime\n",
    "from geneformer import Classifier\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import pickle\n",
    "from huggingface_hub import snapshot_download\n",
    "from utils import plot_confusion, plot_cell_type_distribution, get_high_fraction_celltype_indices\n",
    "\n",
    "# Define the repository ID and local directory\n",
    "repo_id = \"ctheodoris/Geneformer\"\n",
    "local_dir = \".\"\n",
    "\n",
    "# Download all files in the repository\n",
    "snapshot_download(repo_id, local_dir=local_dir, allow_patterns=[\"gf-6L-30M-i2048/*\"])\n",
    "\n",
    "print(f\"All files downloaded to: {local_dir}\")\n",
    "\n",
    "base_url = \"https://huggingface.co/ctheodoris/Geneformer/resolve/main/geneformer/gene_dictionaries_30m/\"\n",
    "files = [\n",
    "    \"ensembl_mapping_dict_gc30M.pkl\",\n",
    "    \"gene_median_dictionary_gc30M.pkl\",\n",
    "    \"gene_name_id_dict_gc30M.pkl\",\n",
    "    \"token_dictionary_gc30M.pkl\"\n",
    "]\n",
    "\n",
    "output_dir = \"./gene_dictionaries_30m\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    output_file = os.path.join(output_dir, file)\n",
    "    if not os.path.exists(output_file):\n",
    "        print(f\"Downloading {file}...\")\n",
    "        urllib.request.urlretrieve(base_url + file, output_file)\n",
    "        print(f\"Downloaded {file}\")\n",
    "    else:\n",
    "        print(f\"{file} already exists.\")\n",
    "\n",
    "# Load the data\n",
    "cell_file = \"data/cells.npy\"\n",
    "cells = np.load(cell_file, allow_pickle=True).ravel()[0]\n",
    "\n",
    "# Extract data\n",
    "expressions = cells[\"UMI\"].toarray()  # Gene expression matrix (n_cells x n_genes)\n",
    "gene_names = cells[\"gene_ids\"]  # Gene names\n",
    "cell_types = cells[\"classes\"]  # Cell types (n_cells,)\n",
    "\n",
    "plot_cell_type_distribution(cell_types)\n",
    "high_fraction_indices = get_high_fraction_celltype_indices(cell_types, 0.05)\n",
    "expressions = expressions[high_fraction_indices]\n",
    "cell_types = cell_types[high_fraction_indices]\n",
    "\n",
    "# Create a DataFrame for stratified sampling\n",
    "cell_df = pd.DataFrame({\"cell_types\": cell_types})\n",
    "\n",
    "# Perform stratified sampling to select 10% of the data\n",
    "_, subsample_indices = train_test_split(\n",
    "    np.arange(len(cell_types)),  # Use indices for subsampling\n",
    "    test_size=0.01,  # 10% subsample\n",
    "    stratify=cell_df[\"cell_types\"],  # Stratify by cell types\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Original dataset size: {len(cell_types)}\")\n",
    "\n",
    "# Subset the data based on sampled indices\n",
    "expressions = expressions[subsample_indices, :]  # Subset expression matrix\n",
    "cell_types = cell_types[subsample_indices]  # Subset cell types\n",
    "# Output sizes for verification\n",
    "print(f\"Subsampled dataset size: {len(cell_types)}\")\n",
    "\n",
    "plot_cell_type_distribution(cell_types)\n",
    "\n",
    "# Example data\n",
    "# Replace `expressions`, `cell_types`, and `gene_names` with your actual data\n",
    "adata = sc.AnnData(X=expressions)\n",
    "adata.obs[\"cell_types\"] = cell_types\n",
    "adata.var_names = gene_names\n",
    "adata.var[\"ensembl_id\"] = gene_names\n",
    "adata.obs[\"n_counts\"] = adata.X.sum(1)  # total read count per cell\n",
    "adata.obs[\"cell_id\"] = adata.obs_names.values\n",
    "\n",
    "# Convert matrix to sparse format if not already\n",
    "if not sp.issparse(adata.X):\n",
    "    adata.X = sp.csr_matrix(adata.X)\n",
    "\n",
    "# Save the AnnData object\n",
    "adata.write_h5ad(\"data/adata.h5ad\")\n",
    "\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "\n",
    "tokenizer = TranscriptomeTokenizer(\n",
    "    custom_attr_name_dict={\"cell_types\": \"cell_types\", \"cell_id\": \"cell_id\"},\n",
    "    model_input_size=2048,  # For 30M model series\n",
    "    special_token=False,   # 30M models require this to be False\n",
    "    gene_median_file=\"./gene_dictionaries_30m/gene_median_dictionary_gc30M.pkl\",\n",
    "    token_dictionary_file=\"./gene_dictionaries_30m/token_dictionary_gc30M.pkl\",\n",
    "    gene_mapping_file=\"./gene_dictionaries_30m/ensembl_mapping_dict_gc30M.pkl\"\n",
    ")\n",
    "\n",
    "tokenizer.tokenize_data(\n",
    "    data_directory=\"./data\",\n",
    "    output_directory=\"./tokenized_data\",\n",
    "    output_prefix=\"my_dataset\",\n",
    "    file_format=\"h5ad\",\n",
    "    use_generator=False\n",
    ")\n",
    "\n",
    "# Example: Map cell types to numeric IDs\n",
    "cell_types_v2 = list(adata.obs[\"cell_types\"].unique())\n",
    "id_class_dict = {i: class_id for i, class_id in enumerate(cell_types_v2)}\n",
    "\n",
    "# Save the dictionary\n",
    "with open(\"./tokenized_data/my_dataset_id_class_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_class_dict, f)\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}{current_date.hour:02d}{current_date.minute:02d}{current_date.second:02d}\"\n",
    "datestamp_min = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "\n",
    "output_prefix = \"cm_classifier_test\"\n",
    "output_dir = f\"output_directory/{datestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "filter_data_dict={\"cell_types\":list(adata.obs[\"cell_types\"].unique())}\n",
    "training_args = {\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"learning_rate\": 0.000804,\n",
    "    \"lr_scheduler_type\": \"polynomial\",\n",
    "    \"warmup_steps\": 1812,\n",
    "    \"weight_decay\":0.258828,\n",
    "    \"per_device_train_batch_size\": 12,\n",
    "    \"seed\": 73,\n",
    "}\n",
    "cc = Classifier(classifier=\"cell\",\n",
    "                cell_state_dict = {\"state_key\": \"cell_types\", \"states\": \"all\"},\n",
    "                filter_data=None, #none = fine tune with all input data\n",
    "                training_args=training_args,\n",
    "                max_ncells=None,\n",
    "                freeze_layers = 2, # freeze the last 2 layer of the model\n",
    "                num_crossval_splits = 1, #only 1 train, test and eval. no cross validation\n",
    "                forward_batch_size=200,\n",
    "                nproc=1)\n",
    "                #rare_threshold=.05)\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    np.arange(len(cell_types)), test_size=0.3, random_state=42, stratify=cell_types\n",
    ")\n",
    "\n",
    "# Step 2: Split temp (30%) into validation (15%) and test (15%)\n",
    "cell_types_temp = cell_types[temp_indices]\n",
    "eval_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=42, stratify=cell_types_temp\n",
    ")\n",
    "\n",
    "# Get cell IDs corresponding to these indices\n",
    "train_ids = adata.obs['cell_id'].iloc[train_indices].tolist()\n",
    "eval_ids = adata.obs['cell_id'].iloc[eval_indices].tolist()\n",
    "test_ids = adata.obs['cell_id'].iloc[test_indices].tolist()\n",
    "\n",
    "# Output the sizes\n",
    "print(f\"Total samples: {len(cell_types)}\")\n",
    "print(f\"Training samples: {len(train_ids)}\")\n",
    "print(f\"Validation samples: {len(eval_ids)}\")\n",
    "print(f\"Test samples: {len(test_ids)}\")\n",
    "\n",
    "\n",
    "# Verify that all classes are present in each split\n",
    "def print_class_distribution(indices, split_name):\n",
    "    split_cell_types = cell_types[indices]\n",
    "    class_counts = Counter(split_cell_types)\n",
    "    print(f\"Class distribution in {split_name} set:\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  {cls}: {count}\")\n",
    "\n",
    "print_class_distribution(train_indices, 'training')\n",
    "print_class_distribution(eval_indices, 'validation')\n",
    "print_class_distribution(test_indices, 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data with correct 'attr_key' and 'train', 'test' IDs\n",
    "train_test_id_split_dict = {\n",
    "    \"attr_key\": \"cell_id\",\n",
    "    \"train\": train_ids + eval_ids,\n",
    "    \"test\": test_ids\n",
    "}\n",
    "\n",
    "\n",
    "# Run prepare_data to create the labeled dataset\n",
    "cc.prepare_data(\n",
    "    input_data_file=\"tokenized_data/my_dataset.dataset\",\n",
    "    output_directory=output_dir,\n",
    "    output_prefix=output_prefix,\n",
    "    split_id_dict=train_test_id_split_dict\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Run prepare_data to create the labeled dataset\n",
    "cc.prepare_data(\n",
    "    input_data_file=\"tokenized_data/my_dataset.dataset\",\n",
    "    output_directory=output_dir,\n",
    "    output_prefix=output_prefix,\n",
    "    split_id_dict=train_test_id_split_dict\n",
    ")\n",
    "\n",
    "# Define the path to save the labeled dataset\n",
    "labeled_dataset_path = f\"{output_dir}/{output_prefix}_labeled_train.dataset\"\n",
    "\n",
    "# Load the dataset from the path\n",
    "labeled_dataset = load_from_disk(labeled_dataset_path)\n",
    "\n",
    "print(\"Columns in labeled dataset:\", labeled_dataset.column_names)\n",
    "assert 'label' in labeled_dataset.column_names, \"The 'label' column is missing in the labeled dataset.\"\n",
    "\n",
    "# Confirm save\n",
    "print(f\"Labeled dataset saved at {labeled_dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the train_valid_id_split_dict to use 'cell_id'\n",
    "train_valid_id_split_dict = {\n",
    "    \"attr_key\": \"cell_id\",\n",
    "    \"train\": train_ids,\n",
    "    \"eval\": eval_ids\n",
    "}\n",
    "\n",
    "\n",
    "test_number = 5\n",
    "output_directory = os.path.abspath(f\"./results/{test_number}\")\n",
    "model_directory = os.path.abspath(\"gf-6L-30M-i2048\")\n",
    "prepared_input_data_file = os.path.abspath(f\"{output_dir}/{output_prefix}_labeled_train.dataset\")\n",
    "id_class_dict_file = os.path.abspath(\"./tokenized_data/my_dataset_id_class_dict.pkl\")\n",
    "os.makedirs(f\"./results/{test_number}\", exist_ok=True)\n",
    "\n",
    "# Validate using the labeled dataset\n",
    "all_metrics = cc.validate(\n",
    "    model_directory=model_directory,\n",
    "    prepared_input_data_file=prepared_input_data_file,\n",
    "    id_class_dict_file=id_class_dict_file,\n",
    "    output_directory=output_directory,\n",
    "    output_prefix=\"my_fine_tuned_model\",\n",
    "    split_id_dict=train_valid_id_split_dict,\n",
    "    n_hyperopt_trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray.tune import ExperimentAnalysis\n",
    "\n",
    "# Convert result_dir to an absolute path\n",
    "result_dir = os.path.abspath(\"results/5/241120_geneformer_cellClassifier_my_fine_tuned_model/ksplit1/_objective_2024-11-20_12-16-10\")\n",
    "\n",
    "# Load experiment analysis\n",
    "analysis = ExperimentAnalysis(result_dir)\n",
    "\n",
    "# Get the best trial based on `eval_macro_f1` (maximize)\n",
    "best_trial = analysis.get_best_trial(metric=\"eval_macro_f1\", mode=\"max\")\n",
    "best_config = analysis.get_best_config(metric=\"eval_macro_f1\", mode=\"max\")\n",
    "best_checkpoint = analysis.get_best_checkpoint(best_trial, metric=\"eval_macro_f1\", mode=\"max\")\n",
    "\n",
    "# Print results\n",
    "print(\"Best trial hyperparameters:\", best_config)\n",
    "print(\"Best checkpoint path:\", best_checkpoint)\n",
    "\n",
    "checkpoint_files_path = os.path.join(best_checkpoint.path, \"checkpoint-34\")\n",
    "print(checkpoint_files_path)\n",
    "\n",
    "from ray.tune.analysis.experiment_analysis import ExperimentAnalysis\n",
    "\n",
    "# Dynamically set them in the Classifier\n",
    "cc = Classifier(\n",
    "    classifier=\"cell\",\n",
    "    cell_state_dict = {\"state_key\": \"cell_types\", \"states\": \"all\"},\n",
    "    forward_batch_size=200,\n",
    "    nproc=1,\n",
    "    training_args=best_config,\n",
    "    #rare_threshold = .05\n",
    ")\n",
    "\n",
    "# Evaluate the saved model using the best checkpoint\n",
    "all_metrics = cc.evaluate_saved_model(\n",
    "    model_directory=os.path.abspath(checkpoint_files_path),\n",
    "    id_class_dict_file=f\"{output_dir}/{output_prefix}_id_class_dict.pkl\",\n",
    "    test_data_file=f\"{output_dir}/{output_prefix}_labeled_test.dataset\",\n",
    "    output_directory=output_dir,\n",
    "    output_prefix=output_prefix,\n",
    "    predict=True,  # Set to False if predictions are not required\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in all_metrics.items():\n",
    "    print(f\"{key}:{val}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for predictions and id-class dictionary\n",
    "predictions_file = os.path.join(output_dir, f\"{output_prefix}_pred_dict.pkl\")\n",
    "id_class_dict_file = os.path.join(output_dir, f\"{output_prefix}_id_class_dict.pkl\")\n",
    "\n",
    "# Verify that the files exist\n",
    "if not os.path.exists(predictions_file):\n",
    "    raise FileNotFoundError(f\"Predictions file not found: {predictions_file}\")\n",
    "if not os.path.exists(id_class_dict_file):\n",
    "    raise FileNotFoundError(f\"ID-Class Dictionary file not found: {id_class_dict_file}\")\n",
    "\n",
    "# Define the custom class order\n",
    "\n",
    "# Plot predictions\n",
    "cc.plot_predictions(\n",
    "    predictions_file=predictions_file,\n",
    "    id_class_dict_file=id_class_dict_file,\n",
    "    title=\"cell type\",\n",
    "    output_directory=output_dir,\n",
    "    output_prefix=output_prefix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'241120'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime(\"%y%m%d\")  # Format: DDMMYY\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
